- Class: meta
  Course: Psychological Statistics You Can Handle
  Lesson: Paired Samples T-Test
  Author: Kevin R. Carriere & Jason Kilgore
  Type: Standard
  Organization: Washington & Jefferson College
  Version: 3.1.0

- Class: script
  Output: Here are some notes for this module. When you've saved the notes, type submit().
  AnswerTests: script_results_identical('saved')
  Hint: Make sure the last line in your script says saved <- "Y".
  Script: Notes.R

- Class: text
  Output: This module focuses on Paired T-Tests (Dependent Samples T-Test or Within-Subjects T-Test).

- Class: text
  Output: For example, we could test your attention before and after giving some exciting stimuli. 

- Class: figure
  Output: This is called a Paired-Sample T-test. It is paired because it's the same sample at two-time points. Remember, a problem with research is that there could be differences between groups. We tried to randomize group assignments to fix this, but we are unsure if our randomization worked. 
  Figure: pairedttest.R
  FigureType: New

- Class: text
  Output: The t statistic we are calculating is taking the mean difference of the pairs, Yd, and subtracting it from some hypothesized true mean value, generally 0 (because we hypothesize no difference), and we divide that by the standard error of the mean differences, defined as the standard deviation divided by the square root of n.
  
- Class: text
  Output: "Paired Samples also reduce the number of people we need to collect. If we wanted two groups of people (some take drugs, some don't), we would need 30 people for each group for a total of 60. But, if we used Paired Samples (first we test you without the drug, then we test you with it), we only need 30 people!"

- Class: text
  Output: "However, there are problems we need to be aware of with Paired Samples or Within-Subjects designs. One is practice effects. If we first test you on something, then give you some treatment, and test you again, sometimes we can't be sure if it's the treatment that improved your score or the fact you took the same test twice."

- Class: text
  Output: "It could also work the other way. You've already taken the test once, so you are tired of taking it and do worse the second time. This is called the fatigue effect."

- Class: text
  Output: "The assumptions of a paired t-test are - \n\n (1) The samples are randomly selected from the population. \n\n (2) The pairs of observations are independent from other pairs. \n\n (3) The difference between the pairs are normally distributed. "

#10
- Class: text
  Output: In this module, we won't test the assumptions - but you should check out the testing non-normality module to learn how to do this. Remember, we want to test the *difference* for paired tests! That might require you to create a new variable.

- Class: text
  Output: "We'll start our foray into Paired T-Tests by loading, not a study this time, but a portion of a large dataset. "

- Class: text
  Output: "Researchers followed participants over three time points during the pandemic. For our purposes, we have loaded the question where researchers asked how effective people perceived wearing an effective mask (1: Not at all to 5: Extremely Effective) to be to stop the spread of COVID-19. "

- Class: text
  Output: The citation for this dataset is - "Pfleger, A., Jensen, E.A., Lorenz, L., Watzlawik, M., Wagoner, B., & Herbig, L. (2021). Viral Communication- Longitudinal Survey Data on the Social Dimensions of the COVID-19 Pandemic [Data set]. Zenodo. doi.org/10.5281/ZENODO.5779516".
  
- Class: text
  Output: We would like to note, before we move on, that this is individual citizens' perceptions of mask-wearing effectiveness - not the actual effectiveness, nor the opinions of scientific experts or epidemiologists. 

- Class: cmd_question
  Output: With that said, let's look at the data a bit. Pipe (|>) the dataset, Masking, into the function head().
  CorrectAnswer: Masking |> head()
  AnswerTests: omnitest(correctExpr='Masking |> head()')
  Hint: Masking |> head()
  
- Class: text
  Output: This data is in what we call the wide form - that is, there are multiple columns for each person. This is the standard form that data is put into for software like SPSS, JASP, or Jamovi. 
  
- Class: text
  Output: You should check out the Pivoting Data module to learn how to change this data into the long form, the 'most tidy' form of data, where each row represents a single observation. Since this data is one person over three time points, in an ideal world, our data would have three rows for each participant, with one column for "Mask Efficiency", one column for "Period", and one column for "Participant ID." 

- Class: text
  Output: However, R can handle analyzing the data for our basic statistics, so we'll go over both ways.

- Class: cmd_question
  Output: So, let's test whether perceptions of masking differed between Time 1 (October to December 2020) and Time 2 (March 2021). To do that, we have to pass our data, Masking, through a function, t.test(), which will take two arguments. The first is going to be another function, called Pair(). Inside of Pair(), we want to pass two arguments, MaskEff_T1 and MaskEff_T2. Outside of Pair(), we want to write ~ 1 . The second argument is data=_. 
  CorrectAnswer: Masking |> t.test(Pair(MaskEff_T1, MaskEff_T2) ~ 1, data=_)
  AnswerTests: omnitest(correctExpr='Masking |> t.test(Pair(MaskEff_T1, MaskEff_T2) ~ 1, data=_)')
  Hint: Masking |> t.test(Pair(MaskEff_T1, MaskEff_T2) ~ 1, data=_)

#20
- Class: mult_question
  Output: As you can see, the mean difference from Time 1 to Time 2 is .2922. Does this mean perceived effectiveness went up or down?
  CorrectAnswer: Down
  AnswerChoices: Up; Down
  AnswerTests: omnitest(correctVal='Down')
  Hint: A larger number subtracted by a smaller number will yield a positive value, while a smaller number subtracted by a larger number will yield a negative value.

- Class: text
  Output: Let's try a second example. 

- Class: text
  Output: The citation for this study is "Mazar, A., & Wood, W. (2022). Illusory Feelings, Elusive habits- People overlook habits in explanations of behavior. Psychological Science, 33(4), 563â€“578. doi.org/10.1177/09567976211045345"

- Class: text
  Output: The psychologists had individuals do a task where they quickly formed a habit of pressing the Z key on their keyboard. After the habit formed, they asked participants if they would be willing to assist for five mores minutes for no additional compensation, where Z was coded to be Yes. 

- Class: text
  Output: After the participants answered, they asked the participants on percentage scales (0% = not at all important to 50% or more = extremely important), the extent to which their decision to help or not was due to habits (I responded automatically, without thinking) and mood, (My mood at the time (I felt good/bad)).

- Class: text
  Output: They report - "To test the perceived effects of habits and mood, we used a dependent-samples t-test to assess the within participants difference in attributions to mood compared with habits. Participants strongly attributed their behavior to mood over habits (mean difference = 17.07, 95% CI = [15.63, 18.51]), t(799) = 23.22, p < .001, d = 1.14, 95% CI = [1.01, 1.26]." 

- Class: cmd_question
  Output: The data is loaded in your environment as habits. Our first step when given data is to do what? Examine what it looks like via summary(), head(), tail(), or View(). Exactly. Do one of those now, remembering to pipe |> your dataframe into one of the functions.
  CorrectAnswer: habits |> summary()
  AnswerTests: any_of_exprs('summary(habits)', 'habits |> summary()', 'head(habits)', 'habits |> head()', 'tail(habits)', 'habits |> tail()', 'View(habits)', 'habits |> View()')
  Hint: Generally, we've been using summary(). So, type habits |> summary().

- Class: cmd_question
  Output: So, was there a difference in how people attributed their habits and mood to helping the researchers? We have our t.test() function, which will take two arguments after we passed our dataset, habits, through it. The first is Pair() ~ 1 , which Pair() needs our variables, mood and habit . The second argument is data=_ .
  CorrectAnswer: habits |> t.test(Pair(mood, habit) ~ 1, data=_)
  AnswerTests: omnitest(correctExpr='habits |> t.test(Pair(mood, habit) ~ 1, data=_)')
  Hint: habits |> t.test(Pair(mood, habit) ~ 1, data=_)

- Class: text
  Output: Before we get into that last part of the sentence - the effect size (d="1.14") - we need to put up some yellow warning lights. 
  
- Class: text
  Output: It turns out it is surprisingly tricky to agree upon a 'correct' effect size for a paired t-test. We talk through and provide some math and code to show this in your Notes, and provide some readings that could be interesting.  

#30
- Class: text
  Output: It comes down to "How do we (or, do we even) account for the fact we've measured the same person twice?" That has complicated implications for effect size calculation.

- Class: text
  Output: What we present is not the 'only' way to calculate effect sizes. We hesitate to even call our method 'correct'. How about we agree that it is simply the one we intend to use in this Module.
  
- Class: cmd_question
  Output: Let's get their effect size. To do this, we're going to do something a bit new - we're going to tell R that we want the package effectsize's version of rm_d(). First, start with our data, habits, and pipe it. We then must write effectsize::rm_d() as the function's name to do that. We will pass it four arguments - Pair(mood, habit) ~ 1, adjust=FALSE, method="rm", and data=_. 
  CorrectAnswer: habits |> effectsize::rm_d(Pair(mood, habit) ~ 1, adjust=FALSE, method="rm", data=_)
  AnswerTests: omnitest(correctExpr='habits |> effectsize::rm_d(Pair(mood, habit) ~ 1, adjust=FALSE, method="rm", data=_)')
  Hint: habits |> effectsize::rm_d(Pair(mood, habit) ~ 1, adjust=FALSE, method="rm", data=_)

- Class: text
  Output: You also could use effsize's cohen.d() to get the same effect size, and we include that in the Notes as well. We believe you should proceed with either rm_d() (Repeated Measures D) or cohen.d() from effsize. 

- Class: cmd_question
  Output: We also want to briefly mention that, ideally, a tidy dataset has one observation per row. Since this is paired, ideally, each person has two rows - one for each response. We've also loaded a tidy dataset - habits_long - in your environment. Check it quickly with the method of your choice.
  CorrectAnswer: habits_long |> summary()
  AnswerTests: any_of_exprs('summary(habits_long)', 'habits_long |> summary()', 'head(habits_long)', 'habits_long |> head()', 'tail(habits_long)', 'habits_long |> tail()', 'View(habits_long)', 'habits_long |> View()')
  Hint: Generally, we've been using summary(). So, type habits_long |> summary().

- Class: cmd_question
  Output: With this long dataset, pipe it into t_test() in the rstatix package. The variables have new names now, but we can set it as a formula. So, two arguments - Response ~ Type and paired=TRUE.
  CorrectAnswer: habits_long |> t_test(Response ~ Type, data=_, paired=TRUE)
  AnswerTests: omnitest(correctExpr='habits_long |> t_test(Response ~ Type, data=_, paired=TRUE)')
  Hint: habits_long |> t_test(Response ~ Type, data=_, paired=TRUE)
#36
- Class: cmd_question
  Output: Great job. Finally, let's pipe habits_long into rm_d(). Things will change now that our data is long and not wide. We will pass it four arguments -- Response ~ Type | rowid, adjust=FALSE, method="rm", data=_. 
  CorrectAnswer: habits_long |> effectsize::rm_d(Response ~ Type | rowid,  adjust=FALSE, method="rm", data=_)
  AnswerTests: omnitest(correctExpr='habits_long |> effectsize::rm_d(Response ~ Type | rowid,  adjust=FALSE, method="rm", data=_)')
  Hint: habits_long |> effectsize::rm_d(Response ~ Type | rowid,  adjust=FALSE, method="rm", data=_)

#37
- Class: text
  Output: In this lesson, we showed you two ways to calculate a within-subjects t-test. The way we recommend is through long datasets and pipe-friendly methods. This would require you to go over the Pivoting Datasets Module. If you don't have your data in the long format (multiple rows per participant), you should default to the clunkier Pair() method. 

